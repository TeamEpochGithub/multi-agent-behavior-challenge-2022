# flake8: noqa
# -*- coding: utf-8 -*-
"""perciver_run.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eSTt1O7Z7Qy1r6X6dbrbxi5zE3OGmfIR
"""
from lib.perciver import perciver

# set up the model
model = perciver.Perceiver(
    input_channels=72,  # number of channels for each token of the input
    input_axis=1,  # number of axis for input data (2 for images, 3 for video)
    num_freq_bands=6,  # number of freq bands, with original value (2 * K + 1)
    max_freq=10.0,  # maximum frequency, hyperparameter depending on how fine the data is
    depth=6,  # depth of net. The shape of the final attention mechanism will be:
    #   depth * (cross attention -> self_per_cross_attn * self attention)
    num_latents=256,  # number of latents, or induced set points, or centroids. different papers giving it different names
    latent_dim=100,  # latent dimension
    cross_heads=1,  # number of heads for cross attention. paper said 1
    latent_heads=8,  # number of heads for latent self attention, 8
    cross_dim_head=64,  # number of dimensions per cross attention head
    latent_dim_head=64,  # number of dimensions per latent self attention head
    num_classes=72,  # output number of classes
    attn_dropout=0.0,
    ff_dropout=0.0,
    weight_tie_layers=False,  # whether to weight tie layers (optional, as indicated in the diagram)
    fourier_encode_data=True,  # whether to auto-fourier encode the data, using the input_axis given. defaults to True, but can be turned off if you are fourier encoding the data yourself
    self_per_cross_attn=2,  # number of self attention blocks per cross attention
)


import random

x_train, x_test, y_train, y_test = [], [], [], []

# generate the training data by stacking the entries of user_train
sequence_keys = list(user_train["sequences"].keys())
num_total_frames = np, sum(
    [seq["keypoints"].shape[0] for _, seq in submission_clips["sequences"].items()]
)
sequence_dim = np.shape(user_train["sequences"][sequence_keys[0]]["keypoints"])
keypoints_dim = sequence_dim[1] * sequence_dim[2] * sequence_dim[3]
print(num_total_frames, sequence_dim, keypoints_dim)

# all of these can be optimize !!!!
kk = 3  # every kkth frame is included
seq_len = 120
stride = 180
look_ahead = 10  # how far in the future to predict


for key in sequence_keys:
    start = 0
    keypoints = user_train["sequences"][key]["keypoints"]
    while start + seq_len * kk + look_ahead < keypoints.shape[0]:
        x = np.array([keypoints[i].flatten() for i in range(start, start + seq_len * kk, kk)])
        y = (keypoints[start + seq_len * kk + look_ahead]).flatten()

        if random.random() < 0.8:
            x_train.append(x)
            y_train.append(y)
        else:
            x_test.append(x)
            y_test.append(y)

        start += stride

np.array(x_train).shape


# throw in keypoints for all three mices
# always a sensible step
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit(np.concatenate(x_train))
x_train = np.array([scaler.transform(x) for x in x_train])
y_train = scaler.transform(y_train)
x_test = np.array([scaler.transform(x) for x in x_test])
y_test = scaler.transform(y_test)


train_dataset = TensorDataset(torch.Tensor(x_train), torch.Tensor(y_train))
test_dataset = TensorDataset(torch.Tensor(x_test), torch.Tensor(y_test))


device = "cuda:0"
model(torch.Tensor(x_train[0:2]).cuda(), return_embeddings=True).shape

from tqdm import tqdm

embed_size = 100
num_total_frames = np.sum(
    [seq["keypoints"].shape[0] for _, seq in submission_clips["sequences"].items()]
)
embeddings_array = np.empty((num_total_frames, embed_size), dtype=np.float32)

frame_number_map = {}
start = 0
for sequence_key in tqdm(submission_clips["sequences"]):
    keypoints = submission_clips["sequences"][sequence_key]["keypoints"]  # to do fill holes
    # if keypoints.size ==0;
    # keypoints = submission_clips["sequences"][sequence_key]["keypoints"]
    embeddings = np.empty((len(keypoints), 100), dtype=np.float32)

    x_start = np.array([keypoints[i].flatten() for i in range(0, seq_len * kk, kk)])
    embs = model(torch.Tensor(x_start).cuda().unsqueeze(0), return_embeddings=True)[0]
    for i in range(len(keypoints)):
        # calc embedding and add them
        if i % 100 == 0 and i + seq_len * kk < len(keypoints):  # update every 100 rather every time
            x_start = np.array([keypoints[i].flatten() for i in range(0, seq - len * kk, kk)])
            embs = model(torch.Tensor(x_start).cuda().unsqueeze(0), return_embeddings=True)[0]

        embeddings[i] = embs.detach().cpu().numpy()

    end = start + len(keypoints)
    embeddings_array[start:end] = embeddings
    frame_number_map[sequence_key] = (start, end)
    start = end

assert end == num_total_frames
submission_dict = {"frame_number_map": frame_number_map, "embeddings": embeddings_array}
